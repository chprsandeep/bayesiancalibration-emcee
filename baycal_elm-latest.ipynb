{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e579b60e",
   "metadata": {},
   "source": [
    "# Bayesian Calibration with Gaussian Process Regression and MCMC Sampling: An Affine-Invariant Ensemble Approach with Gelman-Rubin Convergence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0db3fc",
   "metadata": {},
   "source": [
    "Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7bca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phaniramasandeepchinta/Documents/Anaconda/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/var/folders/bz/tmghl8r56f58nx_wxdd36pv40000gp/T/ipykernel_14199/1508589538.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern, DotProduct, WhiteKernel\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib\n",
    "import emcee\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262678f8",
   "metadata": {},
   "source": [
    "Reading data and GPR fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33cc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-prior function\n",
    "# The prior is defined as a uniform distribution between 0 and 1 for the parameters. \n",
    "# If the parameters (theta) are within this range, the log of the prior probability is 0 (since ln⁡(1)=0ln(1)=0).\n",
    "# If the parameters are outside this range, the log of the prior probability is negative infinity (representing a probability of 0).\n",
    "\n",
    "def lnprior(theta):\n",
    "    if np.all(theta >= 0) and np.all(theta <= 1):\n",
    "        return 0.0  # log(1) for uniform prior\n",
    "    return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c025e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-likelihood function\n",
    "# The likelihood is defined as a Gaussian likelihood centered at desired_nrmse \n",
    "# with a standard deviation of desired_std. This means you're assuming that the error (or discrepancy) \n",
    "# between the predicted NRMSE from your model and the desired NRMSE follows a normal distribution.\n",
    "# The function returns the natural logarithm of this likelihood.\n",
    "\n",
    "def lnlike(theta):\n",
    "    predicted_nrmse = gpr_model.predict([theta])[0]\n",
    "    # Gaussian likelihood centered at desired_nrmse with desired_std\n",
    "    return -0.5 * ((predicted_nrmse - desired_nrmse) / desired_std)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-posterior function - Bayes Theorem\n",
    "# A direct application of the logarithmic form of Bayes' theorem:\n",
    "# ln⁡(Posterior)=ln⁡(Likelihood)+ln⁡(Prior)\n",
    "\n",
    "def lnprob(theta):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43767f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gelman-Rubin diagnostic, often represented as R_hat\n",
    "# R_hat, is a widely used convergence diagnostic for Markov Chain Monte Carlo (MCMC) simulations. \n",
    "# It's based on comparing the variance between multiple independent chains to the variance within each chain. \n",
    "# The idea is that if all chains have converged to the target distribution, the between-chain and \n",
    "# within-chain variances should be similar. If R_hat is close to 1, it suggests that the chains have converged \n",
    "# to the target distribution. \n",
    "# We used convergence if R_hat < 1.1\n",
    "\n",
    "\n",
    "def gelman_rubin(chains):\n",
    "    \"\"\"\n",
    "    Compute the Gelman-Rubin diagnostic (R-hat) for MCMC chains.\n",
    "    \n",
    "    Parameters:\n",
    "    - chains: MCMC chains, shape (n_chains, n_steps, n_parameters)\n",
    "    \n",
    "    Returns:\n",
    "    - R_hat: Gelman-Rubin diagnostic for each parameter\n",
    "    \"\"\"\n",
    "    n_chains, n_steps, n_parameters = chains.shape\n",
    "    # Calculate chain means\n",
    "    chain_means = np.mean(chains, axis=1)\n",
    "    # Calculate overall mean\n",
    "    overall_mean = np.mean(chains, axis=(0, 1))\n",
    "    # Calculate between-chain variance (B/n)\n",
    "    B_over_n = np.sum((chain_means - overall_mean)**2, axis=0) / (n_chains - 1)\n",
    "    # Calculate within-chain variance (W)\n",
    "    W = np.mean(np.var(chains, axis=1, ddof=1), axis=0)\n",
    "    # Estimate of marginal posterior variance (var_hat plus correction)\n",
    "    var_hat_plus = ((n_steps - 1) / n_steps) * W + B_over_n\n",
    "    # Potential scale reduction factor (R-hat)\n",
    "    R_hat = np.sqrt(var_hat_plus / W)\n",
    "    \n",
    "    return R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a30778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output data from the specified file using numpy\n",
    "output_test_data = np.loadtxt(f\"codes/nrmse2/nrmse_train_d250t_12-17.csv\", delimiter=\",\")\n",
    "\n",
    "print(output_test_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1089290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPR model\n",
    "gpr_model = joblib.load(f'codes/best_gpr_model_d250t_12-17.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db80629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set desired NRMSE and standard deviation\n",
    "# We would indeed aim for an NRMSE of zero, as this would indicate perfect agreement between \n",
    "# our model's predictions and the observed data. However, in practice, achieving an NRMSE of zero \n",
    "# might be unrealistic due to various sources of uncertainty (measurement errors, model structural errors, etc.).\n",
    "# Experiment with different desired values of nrmse and std.\n",
    "\n",
    "desired_nrmse = 0.4 #since minimum nrmse from the available samples is 0.403, it is better to take a slighlty lesser value \n",
    "desired_std = 0.02  # tried different combinations but the min nrmse was never going below 0.39; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfac5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the sampler\n",
    "ndim, nwalkers = 5, 50\n",
    "pos = [np.random.rand(ndim) for i in range(nwalkers)]\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "# Run MCMC for some steps\n",
    "nsteps = 20000\n",
    "burnin = int(nsteps // 2)  # discard the first half of the steps\n",
    "sampler.run_mcmc(pos, nsteps)\n",
    "\n",
    "chains_burned = sampler.chain[:, burnin:, :]\n",
    "chains_incl_burned = sampler.chain\n",
    "\n",
    "R_hat_values = gelman_rubin(chains_burned)\n",
    "\n",
    "# Check and print the R_hat values\n",
    "for i, R_hat in enumerate(R_hat_values):\n",
    "    print(f\"Parameter {i+1}: R_hat = {R_hat:.3f}\")\n",
    "    if R_hat > 1.1:\n",
    "        print(f\"Warning: Parameter {i+1} might not have converged (R_hat = {R_hat:.3f}).\")\n",
    "        display(HTML('<span style=\"color: red;\">Stopping because of non-convergence of one or more parameters</span>'))\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61575d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save chains burned\n",
    "joblib.dump(chains_burned, f'codes/saved_models/mcmc_samples_d250t_12-17.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0640f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the burned chains\n",
    "chains_burned = joblib.load(f'codes/saved_models/mcmc_samples_d250t_12-17.pkl')\n",
    "\n",
    "samples = chains_burned.reshape((-1, 5))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the parameter values over iterations\n",
    "# Calculate the mean and standard deviation (or other spread measure) across walkers\n",
    "mean_chains = np.mean(chains_incl_burned, axis=0)\n",
    "std_chains = np.std(chains_incl_burned, axis=0)\n",
    "\n",
    "# Number of parameters\n",
    "num_params = mean_chains.shape[1]\n",
    "\n",
    "# Create a larger figure for all subplots\n",
    "plt.figure(figsize=(10, 4 * num_params))\n",
    "\n",
    "for i in range(num_params):\n",
    "    # Create subplot for each parameter\n",
    "    plt.subplot(num_params, 1, i + 1)\n",
    "    plt.plot(mean_chains[:, i], label=f'Mean Parameter {i+1}')\n",
    "    plt.fill_between(range(mean_chains.shape[0]),\n",
    "                     mean_chains[:, i] - std_chains[:, i],\n",
    "                     mean_chains[:, i] + std_chains[:, i],\n",
    "                     color='lightgrey', alpha=0.5, label=f'Std Dev Parameter {i+1}')\n",
    "    plt.title(f'Summary Trace Plot for Parameter {i+1}')\n",
    "    plt.xlabel('Step Number')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "# Save the entire figure\n",
    "plt.tight_layout()  # Adjusts the plots to fit into the figure area\n",
    "plt.savefig('summary_trace_plots.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots Posterior Distribution of parameters\n",
    "# Parameter names and default values\n",
    "params = ['$Q_{10}$', '$f_{CH_4}$', '$z_τ$', '$f_{D_0}$', '$K_{O_2}$']\n",
    "defval = [0.2, 0.5, 0.57, 0.005, 0.091]\n",
    "\n",
    "# Set global font to Times New Roman\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Plotting the KDE for each parameter with the uniform prior\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))  # 2 rows, 3 columns\n",
    "\n",
    "# Storing handles for legend entries\n",
    "handles, labels = [], []\n",
    "\n",
    "for i in range(5):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    sns.kdeplot(samples[:, i], ax=ax, fill=True, color='skyblue', linewidth=2)\n",
    "    # No need to store handle here, as we'll capture all handles later\n",
    "\n",
    "    ax.axhline(y=1, color='green', linestyle='--')\n",
    "    ax.axvline(x=defval[i], color='red', linestyle=':')\n",
    "\n",
    "    # Adding 2-sigma confidence interval\n",
    "    lower, upper = np.percentile(samples[:, i], [2.5, 97.5])\n",
    "    ax.axvline(x=lower, color='purple', linestyle='-.')\n",
    "    ax.axvline(x=upper, color='purple', linestyle='-.')\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_title(f\"Parameter: {params[i]}\")\n",
    "\n",
    "# Hide the unused subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Adding custom legend elements\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='skyblue', lw=4),\n",
    "                Line2D([0], [0], color='green', linestyle='--'),\n",
    "                Line2D([0], [0], color='red', linestyle=':'),\n",
    "                Line2D([0], [0], color='purple', linestyle='-.')]\n",
    "\n",
    "fig.legend(custom_lines, [\"Posterior\", \"Uniform Prior\", \"Default Value\", \"2σ Interval\"], \n",
    "           loc='center', bbox_to_anchor=(0.8, 0.3), fontsize=14)\n",
    "\n",
    "# Set the xlabel for the last plot in each row\n",
    "axes[0, 0].set_xlabel(\"Normalized Parameter Value\", fontsize=14)\n",
    "axes[0, 1].set_xlabel(\"Normalized Parameter Value\", fontsize=14)\n",
    "axes[0, 2].set_xlabel(\"Normalized Parameter Value\", fontsize=14)\n",
    "axes[1, 0].set_xlabel(\"Normalized Parameter Value\", fontsize=14)\n",
    "axes[1, 1].set_xlabel(\"Normalized Parameter Value\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'pardis_d250t_12-17.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw parameter sets from the posterior distribution\n",
    "n_samples = 50  # or however many you can afford to run\n",
    "selected_samples = samples[np.random.choice(len(samples), n_samples, replace=False)]\n",
    "\n",
    "# Convert to DataFrame\n",
    "#df = pd.DataFrame(selected_samples)\n",
    "\n",
    "# Save to CSV\n",
    "#df.to_csv('selected_samples.csv', index=False)\n",
    "\n",
    "# Run the E3SM Land Model with each parameter set and compute NRMSE for the validation data\n",
    "nrmse_values = []\n",
    "for sample in selected_samples:\n",
    "    # Run the E3SM Land Model with 'sample' as the parameter set\n",
    "    # Compare the model output to the validation data to compute NRMSE\n",
    "    nrmse = gpr_model.predict(sample.reshape(1, -1))  # however you compute NRMSE\n",
    "    nrmse_values.append(nrmse)\n",
    "\n",
    "# Compute summary statistics for the NRMSE values\n",
    "mean_nrmse = np.mean(nrmse_values)\n",
    "lower_bound = np.percentile(nrmse_values, 2.5)\n",
    "upper_bound = np.percentile(nrmse_values, 97.5)\n",
    "\n",
    "print(f\"Mean NRMSE: {mean_nrmse}\")\n",
    "print(f\"2-sigma range: ({lower_bound}, {upper_bound})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
